`torch_dtype` is deprecated! Use `dtype` instead!
Loading model on cuda...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.21it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]
/home/vishnusai/Sali-Cache/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:1598: UserWarning: The following device_map keys do not match any submodules in the model: ['model.image_newline']
  warnings.warn(
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.

Processing 100 frames...
================================================================================

[1/2] Running BASELINE...
Frame 0: cache=784 time=0.813s
Frame 10: cache=784 time=0.057s
Frame 20: cache=784 time=0.048s
Frame 30: cache=784 time=0.048s
Frame 40: cache=784 time=0.050s
Frame 50: cache=784 time=0.048s
Frame 60: cache=784 time=0.048s
Frame 70: cache=784 time=0.048s
Frame 80: cache=784 time=0.048s
Frame 90: cache=784 time=0.048s

[2/2] Running SALI-CACHE with MULTI-LEVEL QUANTIZATION...
Frame 0: cache=784 time=0.051s [prune=0, int4=0, int8=0, fp16=0, fp32=196]
Frame 10: cache=784 time=0.053s [prune=28, int4=34, int8=129, fp16=3, fp32=2]
Frame 20: cache=784 time=0.047s [prune=24, int4=8, int8=55, fp16=63, fp32=46]
Frame 30: cache=784 time=0.048s [prune=42, int4=31, int8=117, fp16=4, fp32=2]
Frame 40: cache=784 time=0.048s [prune=22, int4=11, int8=43, fp16=82, fp32=38]
Frame 50: cache=784 time=0.047s [prune=2, int4=15, int8=148, fp16=23, fp32=8]
Frame 60: cache=784 time=0.048s [prune=0, int4=2, int8=58, fp16=133, fp32=3]
Frame 70: cache=784 time=0.048s [prune=1, int4=1, int8=22, fp16=149, fp32=23]
Frame 80: cache=784 time=0.048s [prune=3, int4=3, int8=13, fp16=70, fp32=107]
Frame 90: cache=784 time=0.048s [prune=1, int4=2, int8=17, fp16=153, fp32=23]

================================================================================
RESULTS SUMMARY
================================================================================

Timing:
  Baseline Avg:   0.0600s per frame
  Sali-Cache Avg: 0.0482s per frame

Multi-Level Policy Distribution:
  Pruned (deleted):  1250 (  6.4%)
  INT4 (4-bit):      1063 (  5.4%)
  INT8 (8-bit):      5833 ( 29.8%)
  FP16 (16-bit):     7282 ( 37.2%)
  FP32 (full):       4172 ( 21.3%)
  ✅ LOW PRUNING (6.4%) - Using graduated compression!

✅ Results saved to results/multilevel_results.json
================================================================================
